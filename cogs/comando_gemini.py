"""
M√≥dulo para la integraci√≥n de Gemini AI con Discord.
Proporciona comandos para interactuar con los modelos de IA de Gemini.
"""
import logging
from typing import List, Optional
from datetime import datetime, timezone
import io
from PIL import Image
import asyncio
from concurrent.futures import ThreadPoolExecutor

import discord
import google.generativeai as genai
from discord.ext import commands
from config.ia_config import (
    text_generation_config,
    safety_settings,
    EMBED_COLORS,
    SUPPORTED_MIME_TYPES,
    BASE_EMBED_COLORS,
    LANGUAGE_MAP,
    GEMINI_TIMEOUT,
    MAX_HISTORY_LENGTH
)
from base.database import (
    get_or_create_gemini_session,
    add_message_to_session,
    get_session_messages,
    reset_gemini_session,
    prune_old_sessions
)

# Configuraci√≥n del logging solo para errores
logging.basicConfig(
    level=logging.ERROR,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ComandoGemini(commands.Cog):
    """Cog para manejar comandos relacionados con Gemini AI."""

    def __init__(self, bot: commands.Bot):
        """
        Inicializa el Cog de Gemini.
        
        Args:
            bot (commands.Bot): Instancia del bot de Discord
        """
        self.bot = bot
        # Aqu√≠ se usa el modelo "gemini-2.0-flash" para el procesamiento de texto
        self.text_model = genai.GenerativeModel(
            model_name="gemini-2.0-flash", 
            generation_config=text_generation_config,
            safety_settings=safety_settings
        )
        # Aqu√≠ se usa el mismo modelo "gemini-2.0-flash" para la comprensi√≥n de im√°genes
        # ya que es multimodal.
        self.multimodal_model = genai.GenerativeModel(
            model_name="gemini-2.0-flash", 
            generation_config=text_generation_config,
            safety_settings=safety_settings
        )
        # Se mantiene un diccionario en memoria como cach√© temporal para evitar excesivas consultas a BD
        self.chat_cache = {}
        # Crear pool de hilos para operaciones bloqueantes
        self.thread_pool = ThreadPoolExecutor(max_workers=5)
        # Inicializar √≠ndice para rotaci√≥n de colores
        self.embed_color_index = 0
        # Limpiar sesiones inactivas (opcional)
        try:
            prune_old_sessions(days_inactive=30)
            logger.info("Se han limpiado sesiones inactivas de m√°s de 30 d√≠as")
        except Exception as e:
            logger.error(f"Error al limpiar sesiones antiguas: {e}", exc_info=True)

    async def cog_unload(self):
        """
        Se llama cuando el cog es descargado.
        Cierra correctamente el ThreadPoolExecutor para liberar recursos.
        """
        if self.thread_pool:
            self.thread_pool.shutdown(wait=True)
            logger.info("ThreadPoolExecutor cerrado correctamente al descargar ComandoGemini.")

    async def _get_user_chat_session(self, user_id: int) -> genai.ChatSession:
        """
        Obtiene o crea una sesi√≥n de chat para un usuario espec√≠fico.
        Utiliza la base de datos para persistencia.
        
        Args:
            user_id (int): ID del usuario de Discord
            
        Returns:
            genai.ChatSession: Sesi√≥n de chat del usuario
        """
        # Si ya est√° en cach√©, la devolvemos directamente
        if user_id in self.chat_cache:
            return self.chat_cache[user_id]
            
        # Obtenemos o creamos la sesi√≥n en la base de datos
        db_session = get_or_create_gemini_session(user_id)
        
        # Recuperamos los mensajes hist√≥ricos de la BD
        db_messages = get_session_messages(db_session.id, limit=MAX_HISTORY_LENGTH)
        
        # Convertimos los mensajes de la BD al formato que espera Gemini API
        history = []
        for msg in db_messages:
            history.append({
                "role": msg.role,
                "parts": [msg.content]
            })
        
        # Iniciamos una nueva sesi√≥n con el historial recuperado
        chat_session = self.text_model.start_chat(history=history)
        
        # Guardamos en cach√© para futuras consultas
        self.chat_cache[user_id] = chat_session
        
        return chat_session

    async def _chunk_and_send(self, ctx: commands.Context, text: str) -> None:
        """
        Divide un mensaje largo en trozos m√°s peque√±os y los env√≠a como embeds con colores alternados.
        
        Args:
            ctx: Contexto del comando
            text: Texto a dividir y enviar
        """
        # Dividir el mensaje en trozos para los embeds (Discord limita a 4096 caracteres por embed)
        chunks = [text[i:i + 4096] for i in range(0, len(text), 4096)]
        
        # Enviar cada trozo como un embed separado, rotando colores
        for i, chunk in enumerate(chunks):
            # Obtener el color actual para la rotaci√≥n
            current_color = BASE_EMBED_COLORS[self.embed_color_index % len(BASE_EMBED_COLORS)]
            # Incrementar el √≠ndice para el siguiente embed
            self.embed_color_index += 1
            
            embed = discord.Embed(
                description=chunk,
                color=current_color,
                timestamp=datetime.now(timezone.utc)
            )
            
            # Solo en el primer embed mostramos el pie con el autor
            if i == 0:
                embed.set_footer(
                    text=f"Solicitado por {ctx.author.display_name}",
                    icon_url=ctx.author.avatar.url if ctx.author.avatar else None
                )
                
            await ctx.send(embed=embed)
    
    async def _process_image(self, image_bytes: bytes):
        """
        Procesa una imagen para enviarla a Gemini.
        Redimensiona la imagen si supera los l√≠mites de tama√±o.
        
        Args:
            image_bytes: Bytes de la imagen a procesar
            
        Returns:
            Image: Objeto de imagen procesado para Gemini
        """
        # Abrir la imagen con PIL
        image = Image.open(io.BytesIO(image_bytes))
        
        # Comprobar si necesitamos redimensionar la imagen
        # El modelo Gemini tiene un l√≠mite de 1024x1024 p√≠xeles
        max_size = 1024
        if image.width > max_size or image.height > max_size:
            # Calcular la proporci√≥n para mantener el aspect ratio
            ratio = min(max_size / image.width, max_size / image.height)
            new_size = (int(image.width * ratio), int(image.height * ratio))
            
            # Redimensionar la imagen
            image = image.resize(new_size, Image.LANCZOS)
            
            # Convertir de nuevo a bytes
            buffer = io.BytesIO()
            image.save(buffer, format="PNG")
            image_bytes = buffer.getvalue()
        
        # Usar el m√©todo directo de genai para crear una imagen desde bytes
        # Este m√©todo es m√°s compatible con diferentes versiones
        return {"mime_type": "image/png", "data": image_bytes}

    async def _run_in_thread(self, func, *args):
        """
        Ejecuta una funci√≥n en un hilo separado y con timeout.
        
        Args:
            func: La funci√≥n a ejecutar
            args: Argumentos para la funci√≥n
            
        Returns:
            El resultado de la funci√≥n
            
        Raises:
            asyncio.TimeoutError: Si la funci√≥n tarda m√°s del timeout definido
        """
        # Usamos loop.run_in_executor para ejecutar la funci√≥n en un hilo separado
        loop = asyncio.get_event_loop()
        try:
            return await asyncio.wait_for(
                loop.run_in_executor(self.thread_pool, func, *args),
                timeout=GEMINI_TIMEOUT
            )
        except asyncio.TimeoutError:
            logger.warning(f"Timeout al ejecutar funci√≥n {func.__name__}")
            raise
        except Exception as e:
            logger.error(f"Error inesperado al ejecutar {func.__name__} en un hilo separado: {e}", exc_info=True)
            raise
    
    def _prepare_localized_prompt(self, prompt: str, lang_code: str, is_image: bool = False) -> str:
        """
        Asegura que el prompt solicite una respuesta en el idioma especificado.
        
        Args:
            prompt (str): Prompt original del usuario
            lang_code (str): C√≥digo del idioma (ej: 'es', 'en', etc.)
            is_image (bool): Si es True, utiliza un prompt predeterminado para im√°genes cuando est√° vac√≠o
            
        Returns:
            str: Prompt modificado para asegurar respuesta en el idioma solicitado
        """
        # Obtener el nombre del idioma del mapa de idiomas, o espa√±ol por defecto
        language_name = LANGUAGE_MAP.get(lang_code.lower(), "espa√±ol")
        
        # Si no hay prompt, devolver un prompt predeterminado en el idioma solicitado
        if not prompt or prompt.strip() == "":
            if is_image:
                # Para im√°genes, solicitar descripci√≥n en el idioma correspondiente
                if lang_code == "es":
                    return f"Describe esta imagen en espa√±ol"
                else:
                    return f"Describe esta imagen en {language_name}" if lang_code == "es" else f"Describe this image in {language_name}"
            else:
                # Para chat de texto, saludar en el idioma correspondiente
                if lang_code == "es":
                    return "Hola, responde en espa√±ol por favor."
                elif lang_code == "en":
                    return "Hello, please respond in English."
                else:
                    return f"Hello, please respond in {language_name}."
                    
        # Si ya hay un prompt, a√±adir instrucci√≥n sobre el idioma si no est√° presente ya
        if language_name.lower() not in prompt.lower():
            if lang_code == "es":
                return f"{prompt} (Responde en espa√±ol)"
            elif lang_code == "en":
                return f"{prompt} (Respond in English)"
            else:
                return f"{prompt} (Respond in {language_name})"
                
        return prompt

    @commands.command(name='gemini')
    async def gemini_command(self, ctx: commands.Context, *, prompt: str = ""):
        """
        Comando principal para interactuar con Gemini AI.
        Puede procesar texto y, opcionalmente, im√°genes adjuntas.
        Uso: >gemini [--lang <c√≥digo>] <tu pregunta> (adjunta una imagen si quieres an√°lisis visual)
        Ejemplo: >gemini --lang en How's the weather?
        
        Args:
            ctx (commands.Context): Contexto del comando
            prompt (str): Prompt del usuario, puede incluir --lang <c√≥digo> para especificar idioma
        """
        # Extraer el par√°metro de idioma si est√° presente
        lang_code = "es"  # Idioma por defecto: espa√±ol
        
        # Buscar el par√°metro --lang en el prompt
        if prompt and "--lang" in prompt.lower():
            parts = prompt.split()
            for i, part in enumerate(parts):
                if part.lower() == "--lang" and i + 1 < len(parts):
                    potential_lang = parts[i + 1].lower()
                    if potential_lang in LANGUAGE_MAP:
                        lang_code = potential_lang
                        # Eliminar --lang y el c√≥digo de idioma del prompt
                        parts.pop(i)  # Eliminar --lang
                        parts.pop(i)  # Eliminar el c√≥digo de idioma
                        prompt = " ".join(parts)
                        break
        
        # Enviamos un mensaje de "pensando" con un embed profesional
        thinking_embed = discord.Embed(
            title="üß† Procesando consulta...",
            description="**Pythonbot** est√° pensando tu respuesta. Por favor espera un momento.",
            color=EMBED_COLORS["default"]
        )
        thinking_message = await ctx.send(embed=thinking_embed)

        attached_image = None
        if ctx.message.attachments:
            for attachment in ctx.message.attachments:
                # Comprobar si el adjunto es una imagen
                if attachment.content_type and any(mime_type in attachment.content_type for mime_type in SUPPORTED_MIME_TYPES):
                    try:
                        image_bytes = await attachment.read()
                        attached_image = await self._process_image(image_bytes)
                        break # Solo procesamos la primera imagen adjunta
                    except Exception as e:
                        logger.error(f"Error al procesar la imagen adjunta: {e}")
                        await thinking_message.delete()
                        await ctx.send("Hubo un error al procesar la imagen. Por favor, intenta de nuevo.")
                        return

        try:
            if attached_image:
                # Si hay una imagen, enviamos el prompt y la imagen al modelo multimodal
                # Aseguramos que responda en el idioma solicitado
                localized_prompt = self._prepare_localized_prompt(prompt, lang_code, is_image=True)
                
                try:
                    # Ejecutar la solicitud en un hilo separado con timeout
                    response = await self._run_in_thread(
                        self.multimodal_model.generate_content, 
                        [localized_prompt, attached_image]
                    )
                    
                    # Guardar el mensaje del usuario en la BD (solo el prompt, no podemos guardar la imagen)
                    db_session = get_or_create_gemini_session(ctx.author.id)
                    add_message_to_session(db_session.id, "user", localized_prompt)
                    
                    # Guardar la respuesta del modelo
                    response_text = response.text
                    add_message_to_session(db_session.id, "model", response_text)
                    
                except asyncio.TimeoutError:
                    await thinking_message.delete()
                    await ctx.send("La respuesta est√° tardando demasiado. Por favor, intenta con una consulta m√°s simple o int√©ntalo m√°s tarde.")
                    return
            else:
                # Si no hay imagen, usamos el chat de texto
                chat_session = await self._get_user_chat_session(ctx.author.id)
                
                # Preparamos el prompt con el idioma solicitado
                localized_prompt = self._prepare_localized_prompt(prompt, lang_code)
                
                try:
                    # Ejecutar la solicitud en un hilo separado con timeout
                    response = await self._run_in_thread(
                        chat_session.send_message,
                        localized_prompt
                    )
                    
                    # Guardar mensajes en la BD
                    db_session = get_or_create_gemini_session(ctx.author.id)
                    add_message_to_session(db_session.id, "user", localized_prompt)
                    add_message_to_session(db_session.id, "model", response.text)
                    
                except asyncio.TimeoutError:
                    await thinking_message.delete()
                    await ctx.send("La respuesta est√° tardando demasiado. Por favor, intenta con una consulta m√°s simple o int√©ntalo m√°s tarde.")
                    return

            # Eliminar el mensaje de "pensando"
            await thinking_message.delete()
            
            # Enviar la respuesta usando el nuevo m√©todo de embeds coloridos
            await self._chunk_and_send(ctx, response.text)
            
        except ValueError as e:
            # Manejar errores espec√≠ficos de la API
            await thinking_message.delete()
            error_message = str(e).lower()
            
            if "blocked" in error_message:
                await ctx.send(
                    "Tu consulta ha sido bloqueada debido a restricciones de contenido. " +
                    "Por favor, reformula tu pregunta de manera m√°s apropiada."
                )
            else:
                await ctx.send(
                    f"Ha ocurrido un error al procesar tu consulta: {str(e)[:100]}... " +
                    "Por favor, intenta reformular tu pregunta."
                )
        except Exception as e:
            # Manejar cualquier otro error
            logger.error(f"Error al procesar la solicitud de Gemini: {e}", exc_info=True)
            await thinking_message.delete()
            await ctx.send(
                "Ha ocurrido un error inesperado al procesar tu consulta. " +
                f"Por favor, intenta de nuevo m√°s tarde. (Error: {str(e)[:100]})"
            )

    @commands.command(name='gemini_reset')
    async def reset_gemini_command(self, ctx: commands.Context):
        """
        Reinicia la sesi√≥n de chat con Gemini AI para el usuario.
        Uso: >gemini_reset
        
        Args:
            ctx (commands.Context): Contexto del comando
        """
        # Reiniciamos la sesi√≥n en BD
        reset_gemini_session(ctx.author.id)
        
        # Eliminamos la cach√©
        if ctx.author.id in self.chat_cache:
            del self.chat_cache[ctx.author.id]
            
        await ctx.send("He olvidado nuestra conversaci√≥n anterior. ¬°Empecemos de nuevo!")

    @commands.command(name='gemini_help')
    async def gemini_help_command(self, ctx: commands.Context):
        """
        Muestra ayuda sobre el uso del comando gemini y sus opciones.
        Uso: >gemini_help
        
        Args:
            ctx (commands.Context): Contexto del comando
        """
        # Crear un embed colorido con la informaci√≥n de ayuda
        help_embed = discord.Embed(
            title="ü§ñ Ayuda de Gemini AI",
            description="Gemini es un modelo de IA avanzado que puede responder preguntas, analizar im√°genes y mantener conversaciones.",
            color=BASE_EMBED_COLORS[0]
        )
        
        # Comandos disponibles
        help_embed.add_field(
            name="üìù Comandos disponibles",
            value=(
                "**>gemini** [--lang c√≥digo] *pregunta*\n"
                "Realiza una consulta a Gemini. Puedes adjuntar una imagen.\n\n"
                "**>gemini_reset**\n"
                "Reinicia tu conversaci√≥n con Gemini.\n\n"
                "**>gemini_help**\n"
                "Muestra esta ayuda."
            ),
            inline=False
        )
        
        # Opciones de idioma
        languages = ", ".join([f"`{code}`" for code in LANGUAGE_MAP.keys()])
        help_embed.add_field(
            name="üåê Idiomas soportados",
            value=(
                f"Puedes especificar el idioma de respuesta con `--lang c√≥digo`.\n"
                f"C√≥digos disponibles: {languages}\n"
                f"Ejemplo: `>gemini --lang en What's the weather like?`"
            ),
            inline=False
        )
        
        # Consejos de uso
        help_embed.add_field(
            name="üí° Consejos",
            value=(
                "‚Ä¢ Para an√°lisis de im√°genes, adjunta una imagen a tu mensaje.\n"
                "‚Ä¢ S√© espec√≠fico en tus preguntas para obtener mejores respuestas.\n"
                "‚Ä¢ Si no especificas un idioma, Gemini responder√° en espa√±ol por defecto."
            ),
            inline=False
        )
        
        help_embed.set_footer(text="Gemini AI - Powered by Google")
        
        # Enviar el mensaje y configurarlo para que se borre despu√©s de 60 segundos
        await ctx.message.delete(delay=60)  # Borra el mensaje del usuario despu√©s de 60 segundos
        help_message = await ctx.send(embed=help_embed)
        await help_message.delete(delay=60)  # Borra la respuesta despu√©s de 60 segundos

async def setup(bot: commands.Bot):
    """
    Configura el cog de Gemini en el bot.
    
    Args:
        bot (commands.Bot): Instancia del bot
    """
    await bot.add_cog(ComandoGemini(bot))
