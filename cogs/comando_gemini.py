"""
M√≥dulo para la integraci√≥n de DeepSeek AI con Discord.
Proporciona comandos para interactuar con los modelos de IA de DeepSeek.
"""

import asyncio
import io
import logging
import os
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timezone

import discord
from discord.ext import commands
from openai import OpenAI
from PIL import Image

from base.database import (
    add_message_to_session,
    get_or_create_gemini_session,
    get_session_messages,
    prune_old_sessions,
    reset_gemini_session,
)
from config.ia_config import (
    BASE_EMBED_COLORS,
    DEEPSEEK_TIMEOUT,
    EMBED_COLORS,
    LANGUAGE_MAP,
    MAX_HISTORY_LENGTH,
    SUPPORTED_MIME_TYPES,
)

# Configuraci√≥n del logging solo para errores
logging.basicConfig(
    level=logging.ERROR, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class ComandoGemini(commands.Cog):
    """Cog para manejar comandos relacionados con DeepSeek AI."""

    def __init__(self, bot: commands.Bot):
        """
        Inicializa el Cog de DeepSeek.

        Args:
            bot (commands.Bot): Instancia del bot de Discord
        """
        self.bot = bot
        # Inicializar cliente DeepSeek (usa API compatible con OpenAI)
        deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
        if not deepseek_api_key:
            logger.error(
                "DEEPSEEK_API_KEY no est√° configurada en las variables de entorno"
            )
            raise ValueError("DEEPSEEK_API_KEY es requerida")

        self.client = OpenAI(
            api_key=deepseek_api_key, base_url="https://api.deepseek.com"
        )
        self.model_name = "deepseek-chat"
        # Se mantiene un diccionario en memoria como cach√© temporal para evitar excesivas consultas a BD
        self.chat_cache: dict[int, list] = {}
        # Crear pool de hilos para operaciones bloqueantes
        self.thread_pool = ThreadPoolExecutor(max_workers=5)
        # Inicializar √≠ndice para rotaci√≥n de colores
        self.embed_color_index = 0
        # Limpiar sesiones inactivas (opcional)
        try:
            prune_old_sessions(days_inactive=30)
            logger.info("Se han limpiado sesiones inactivas de m√°s de 30 d√≠as")
        except Exception as e:
            logger.error(f"Error al limpiar sesiones antiguas: {e}", exc_info=True)

    async def cog_unload(self):
        """
        Se llama cuando el cog es descargado.
        Cierra correctamente el ThreadPoolExecutor para liberar recursos.
        """
        if self.thread_pool:
            self.thread_pool.shutdown(wait=True)
            logger.info(
                "ThreadPoolExecutor cerrado correctamente al descargar ComandoGemini."
            )

    async def _get_user_chat_session(self, user_id: int) -> list:
        """
        Obtiene o crea una sesi√≥n de chat para un usuario espec√≠fico.
        Utiliza la base de datos para persistencia.

        Args:
            user_id (int): ID del usuario de Discord

        Returns:
            list: Historial de mensajes del usuario en formato OpenAI
        """
        # Si ya est√° en cach√©, la devolvemos directamente
        if user_id in self.chat_cache:
            return self.chat_cache[user_id]

        # Obtenemos o creamos la sesi√≥n en la base de datos
        db_session = get_or_create_gemini_session(user_id)

        # Recuperamos los mensajes hist√≥ricos de la BD
        db_messages = get_session_messages(db_session.id, limit=MAX_HISTORY_LENGTH)

        # Convertimos los mensajes de la BD al formato que espera OpenAI API
        history = []
        for msg in db_messages:
            # DeepSeek usa 'user' y 'assistant' como roles
            role = "assistant" if msg.role == "model" else "user"
            history.append({"role": role, "content": msg.content})

        # Guardamos en cach√© para futuras consultas
        self.chat_cache[user_id] = history

        return history

    async def _chunk_and_send(self, ctx: commands.Context, text: str) -> None:
        """
        Divide un mensaje largo en trozos m√°s peque√±os y los env√≠a como embeds con colores alternados.

        Args:
            ctx: Contexto del comando
            text: Texto a dividir y enviar
        """
        # Dividir el mensaje en trozos para los embeds (Discord limita a 4096 caracteres por embed)
        chunks = [text[i : i + 4096] for i in range(0, len(text), 4096)]

        # Enviar cada trozo como un embed separado, rotando colores
        for i, chunk in enumerate(chunks):
            # Obtener el color actual para la rotaci√≥n
            current_color = BASE_EMBED_COLORS[
                self.embed_color_index % len(BASE_EMBED_COLORS)
            ]
            # Incrementar el √≠ndice para el siguiente embed
            self.embed_color_index += 1

            # Solo el √∫ltimo embed tendr√° timestamp y footer
            if i == len(chunks) - 1:  # √öltimo embed
                embed = discord.Embed(
                    description=chunk,
                    color=current_color,
                    timestamp=datetime.now(timezone.utc),
                )
            else:
                embed = discord.Embed(description=chunk, color=current_color)

            # Solo en el √öLTIMO embed mostramos el pie con el autor
            if i == len(chunks) - 1:  # √öltimo embed
                embed.set_footer(
                    text=f"Solicitado por {ctx.author.display_name}",
                    icon_url=ctx.author.avatar.url if ctx.author.avatar else None,
                )

            await ctx.send(embed=embed)

    async def _process_image(self, image_bytes: bytes):
        """
        Procesa una imagen para enviarla a Gemini.
        Redimensiona la imagen si supera los l√≠mites de tama√±o.

        Args:
            image_bytes: Bytes de la imagen a procesar

        Returns:
            Image: Objeto de imagen procesado para Gemini
        """
        # Abrir la imagen con PIL
        image = Image.open(io.BytesIO(image_bytes))

        # Comprobar si necesitamos redimensionar la imagen
        # El modelo Gemini tiene un l√≠mite de 1024x1024 p√≠xeles
        max_size = 1024
        if image.width > max_size or image.height > max_size:
            # Calcular la proporci√≥n para mantener el aspect ratio
            ratio = min(max_size / image.width, max_size / image.height)
            new_size = (int(image.width * ratio), int(image.height * ratio))

            # Redimensionar la imagen
            image = image.resize(new_size, Image.Resampling.LANCZOS)

            # Convertir de nuevo a bytes
            buffer = io.BytesIO()
            image.save(buffer, format="PNG")
            image_bytes = buffer.getvalue()

        # Usar el m√©todo directo de genai para crear una imagen desde bytes
        # Este m√©todo es m√°s compatible con diferentes versiones
        return {"mime_type": "image/png", "data": image_bytes}

    async def _run_in_thread(self, func, *args):
        """
        Ejecuta una funci√≥n en un hilo separado y con timeout.

        Args:
            func: La funci√≥n a ejecutar
            args: Argumentos para la funci√≥n

        Returns:
            El resultado de la funci√≥n

        Raises:
            asyncio.TimeoutError: Si la funci√≥n tarda m√°s del timeout definido
        """
        # Usamos loop.run_in_executor para ejecutar la funci√≥n en un hilo separado
        loop = asyncio.get_event_loop()
        retries = 3
        for attempt in range(retries):
            try:
                return await asyncio.wait_for(
                    loop.run_in_executor(self.thread_pool, func, *args),
                    timeout=DEEPSEEK_TIMEOUT,
                )
            except asyncio.TimeoutError:
                logger.warning(
                    f"Timeout al ejecutar funci√≥n {func.__name__} (intento {attempt + 1}/{retries})"
                )
                if attempt < retries - 1:
                    await asyncio.sleep(2**attempt)  # Espera exponencial
                else:
                    raise
            except Exception as e:
                error_message = str(e)
                if (
                    "429 You exceeded your current quota" in error_message
                    and attempt < retries - 1
                ):
                    logger.warning(
                        f"Error 429 (Cuota excedida) al ejecutar {func.__name__}. Reintentando en {2**attempt} segundos..."
                    )
                    await asyncio.sleep(2**attempt)  # Espera exponencial
                else:
                    logger.error(
                        f"Error inesperado al ejecutar {func.__name__} en un hilo separado: {e}",
                        exc_info=True,
                    )
                    raise

    def _prepare_localized_prompt(
        self, prompt: str, lang_code: str, is_image: bool = False
    ) -> str:
        """
        Asegura que el prompt solicite una respuesta en el idioma especificado.

        Args:
            prompt (str): Prompt original del usuario
            lang_code (str): C√≥digo del idioma (ej: 'es', 'en', etc.)
            is_image (bool): Si es True, utiliza un prompt predeterminado para im√°genes cuando est√° vac√≠o

        Returns:
            str: Prompt modificado para asegurar respuesta en el idioma solicitado
        """
        # Obtener el nombre del idioma del mapa de idiomas, o espa√±ol por defecto
        language_name = LANGUAGE_MAP.get(lang_code.lower(), "espa√±ol")

        # Si no hay prompt, devolver un prompt predeterminado en el idioma solicitado
        if not prompt or prompt.strip() == "":
            if is_image:
                # Para im√°genes, solicitar descripci√≥n en el idioma correspondiente
                if lang_code == "es":
                    return "Describe esta imagen en espa√±ol"
                else:
                    return (
                        f"Describe esta imagen en {language_name}"
                        if lang_code == "es"
                        else f"Describe this image in {language_name}"
                    )
            else:
                # Para chat de texto, saludar en el idioma correspondiente
                if lang_code == "es":
                    return "Hola, responde en espa√±ol por favor."
                elif lang_code == "en":
                    return "Hello, please respond in English."
                else:
                    return f"Hello, please respond in {language_name}."

        # Si ya hay un prompt, a√±adir instrucci√≥n sobre el idioma si no est√° presente ya
        if language_name.lower() not in prompt.lower():
            if lang_code == "es":
                return f"{prompt} (Responde en espa√±ol)"
            elif lang_code == "en":
                return f"{prompt} (Respond in English)"
            else:
                return f"{prompt} (Respond in {language_name})"

        return prompt

    @commands.command(name="deepseek")
    async def deepseek_command(self, ctx: commands.Context, *, prompt: str = ""):
        """
        Comando principal para interactuar con DeepSeek AI.
        Puede procesar texto y, opcionalmente, im√°genes adjuntas.
        Uso: >deepseek [--lang <c√≥digo>] <tu pregunta> (adjunta una imagen si quieres an√°lisis visual)
        Ejemplo: >deepseek --lang en How's the weather?

        Args:
            ctx (commands.Context): Contexto del comando
            prompt (str): Prompt del usuario, puede incluir --lang <c√≥digo> para especificar idioma
        """
        # Extraer el par√°metro de idioma si est√° presente
        lang_code = "es"  # Idioma por defecto: espa√±ol

        # Buscar el par√°metro --lang en el prompt
        if prompt and "--lang" in prompt.lower():
            parts = prompt.split()
            for i, part in enumerate(parts):
                if part.lower() == "--lang" and i + 1 < len(parts):
                    potential_lang = parts[i + 1].lower()
                    if potential_lang in LANGUAGE_MAP:
                        lang_code = potential_lang
                        # Eliminar --lang y el c√≥digo de idioma del prompt
                        parts.pop(i)  # Eliminar --lang
                        parts.pop(i)  # Eliminar el c√≥digo de idioma
                        prompt = " ".join(parts)
                        break

        # Enviamos un mensaje de "pensando" con un embed profesional
        thinking_embed = discord.Embed(
            title="ÔøΩ Procesando consulta...",
            description="**DeepSeek AI** est√° pensando tu respuesta. Por favor espera un momento.",
            color=EMBED_COLORS["default"],
        )
        thinking_message = await ctx.send(embed=thinking_embed)

        attached_image = None
        if ctx.message.attachments:
            for attachment in ctx.message.attachments:
                # Comprobar si el adjunto es una imagen
                if attachment.content_type and any(
                    mime_type in attachment.content_type
                    for mime_type in SUPPORTED_MIME_TYPES
                ):
                    try:
                        image_bytes = await attachment.read()
                        attached_image = await self._process_image(image_bytes)
                        break  # Solo procesamos la primera imagen adjunta
                    except Exception as e:
                        logger.error(f"Error al procesar la imagen adjunta: {e}")
                        await thinking_message.delete()
                        await ctx.send(
                            "Hubo un error al procesar la imagen. Por favor, intenta de nuevo."
                        )
                        return

        try:
            if attached_image:
                # Si hay una imagen, enviamos el prompt y la imagen al modelo multimodal
                # Aseguramos que responda en el idioma solicitado
                localized_prompt = self._prepare_localized_prompt(
                    prompt, lang_code, is_image=True
                )

                try:
                    # DeepSeek soporta visi√≥n con deepseek-chat
                    # Convertir imagen a base64 para enviar
                    import base64

                    image_base64 = base64.b64encode(attached_image["data"]).decode(
                        "utf-8"
                    )

                    messages = [
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": localized_prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{image_base64}"
                                    },
                                },
                            ],
                        }
                    ]

                    # Ejecutar la solicitud en un hilo separado con timeout
                    def call_deepseek():
                        return self.client.chat.completions.create(
                            model=self.model_name,
                            messages=messages,
                            temperature=0.9,
                            max_tokens=2000,
                        )

                    response = await self._run_in_thread(call_deepseek)

                    # Guardar el mensaje del usuario en la BD
                    db_session = get_or_create_gemini_session(ctx.author.id)
                    add_message_to_session(db_session.id, "user", localized_prompt)

                    # Guardar la respuesta del modelo
                    response_text = response.choices[0].message.content
                    add_message_to_session(db_session.id, "model", response_text)

                except asyncio.TimeoutError:
                    await thinking_message.delete()
                    await ctx.send(
                        "La respuesta est√° tardando demasiado. Por favor, intenta con una consulta m√°s simple o int√©ntalo m√°s tarde."
                    )
                    return
            else:
                # Si no hay imagen, usamos el chat de texto
                history = await self._get_user_chat_session(ctx.author.id)

                # Preparamos el prompt con el idioma solicitado
                localized_prompt = self._prepare_localized_prompt(prompt, lang_code)

                try:
                    # Agregar el mensaje del usuario al historial
                    messages = history + [{"role": "user", "content": localized_prompt}]

                    # Ejecutar la solicitud en un hilo separado con timeout
                    def call_deepseek():
                        return self.client.chat.completions.create(
                            model=self.model_name,
                            messages=messages,
                            temperature=0.9,
                            max_tokens=2000,
                        )

                    response = await self._run_in_thread(call_deepseek)

                    # Guardar mensajes en la BD
                    db_session = get_or_create_gemini_session(ctx.author.id)
                    add_message_to_session(db_session.id, "user", localized_prompt)
                    response_text = response.choices[0].message.content
                    add_message_to_session(db_session.id, "model", response_text)

                    # Actualizar cach√© con la nueva respuesta
                    self.chat_cache[ctx.author.id] = messages + [
                        {"role": "assistant", "content": response_text}
                    ]

                except asyncio.TimeoutError:
                    await thinking_message.delete()
                    await ctx.send(
                        "La respuesta est√° tardando demasiado. Por favor, intenta con una consulta m√°s simple o int√©ntalo m√°s tarde."
                    )
                    return

            # Eliminar el mensaje de "pensando"
            await thinking_message.delete()

            # Enviar la respuesta usando el nuevo m√©todo de embeds coloridos
            await self._chunk_and_send(ctx, response_text)

        except ValueError as e:
            # Manejar errores espec√≠ficos de la API
            await thinking_message.delete()
            error_message = str(e).lower()

            if "blocked" in error_message:
                await ctx.send(
                    "Tu consulta ha sido bloqueada debido a restricciones de contenido. "
                    + "Por favor, reformula tu pregunta de manera m√°s apropiada."
                )
            else:
                await ctx.send(
                    f"Ha ocurrido un error al procesar tu consulta: {str(e)[:100]}... "
                    + "Por favor, intenta reformular tu pregunta."
                )
        except Exception as e:
            # Manejar cualquier otro error
            logger.error(
                f"Error al procesar la solicitud de Gemini: {e}", exc_info=True
            )
            await thinking_message.delete()
            await ctx.send(
                "Ha ocurrido un error inesperado al procesar tu consulta. "
                + f"Por favor, intenta de nuevo m√°s tarde. (Error: {str(e)[:100]})"
            )

    @commands.command(name="deepseek_reset")
    async def reset_deepseek_command(self, ctx: commands.Context):
        """
        Reinicia la sesi√≥n de chat con DeepSeek AI para el usuario.
        Uso: >deepseek_reset

        Args:
            ctx (commands.Context): Contexto del comando
        """
        # Reiniciamos la sesi√≥n en BD
        reset_gemini_session(ctx.author.id)

        # Eliminamos la cach√©
        if ctx.author.id in self.chat_cache:
            del self.chat_cache[ctx.author.id]

        await ctx.send(
            "‚ú® He olvidado nuestra conversaci√≥n anterior. ¬°Empecemos de nuevo!"
        )

    @commands.command(name="deepseek_help")
    async def deepseek_help_command(self, ctx: commands.Context):
        """
        Muestra ayuda sobre el uso del comando deepseek y sus opciones.
        Uso: >deepseek_help

        Args:
            ctx (commands.Context): Contexto del comando
        """
        # Crear un embed colorido con la informaci√≥n de ayuda
        help_embed = discord.Embed(
            title="ü§ñ Ayuda de DeepSeek AI",
            description="DeepSeek es un modelo de IA avanzado que puede responder preguntas, analizar im√°genes y mantener conversaciones en espa√±ol.",
            color=BASE_EMBED_COLORS[0],
        )

        # Comandos disponibles
        help_embed.add_field(
            name="üìù Comandos disponibles",
            value=(
                "**>deepseek** [--lang c√≥digo] *pregunta*\n"
                "Realiza una consulta a DeepSeek AI. Puedes adjuntar una imagen.\n\n"
                "**>deepseek_reset**\n"
                "Reinicia tu conversaci√≥n con DeepSeek.\n\n"
                "**>deepseek_help**\n"
                "Muestra esta ayuda."
            ),
            inline=False,
        )

        # Opciones de idioma
        languages = ", ".join([f"`{code}`" for code in LANGUAGE_MAP.keys()])
        help_embed.add_field(
            name="üåê Idiomas soportados",
            value=(
                f"Puedes especificar el idioma de respuesta con `--lang c√≥digo`.\n"
                f"C√≥digos disponibles: {languages}\n"
                f"Ejemplo: `>deepseek --lang en What's the weather like?`\n"
                f"Por defecto, DeepSeek responde en espa√±ol."
            ),
            inline=False,
        )

        # Consejos de uso
        help_embed.add_field(
            name="üí° Consejos",
            value=(
                "‚Ä¢ Para an√°lisis de im√°genes, adjunta una imagen a tu mensaje.\n"
                "‚Ä¢ S√© espec√≠fico en tus preguntas para obtener mejores respuestas.\n"
                "‚Ä¢ DeepSeek es econ√≥mico y sin l√≠mites de cuota.\n"
                "‚Ä¢ Responde perfectamente en espa√±ol y otros idiomas."
            ),
            inline=False,
        )

        help_embed.set_footer(text="DeepSeek AI - Modelo avanzado de razonamiento")

        # Enviar el mensaje y configurarlo para que se borre despu√©s de 60 segundos
        await ctx.message.delete(
            delay=60
        )  # Borra el mensaje del usuario despu√©s de 60 segundos
        help_message = await ctx.send(embed=help_embed)
        await help_message.delete(delay=60)  # Borra la respuesta despu√©s de 60 segundos


async def setup(bot: commands.Bot):
    """
    Configura el cog de DeepSeek en el bot.

    Args:
        bot (commands.Bot): Instancia del bot
    """
    await bot.add_cog(ComandoGemini(bot))
